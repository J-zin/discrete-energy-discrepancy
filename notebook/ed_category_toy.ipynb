{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sympy.combinatorics.graycode import GrayCode\n",
    "torch.cuda.set_device(7)\n",
    "\n",
    "def inf_train_gen(data, rng=None, batch_size=200):\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState()\n",
    "\n",
    "    if data == \"swissroll\":\n",
    "        data = sklearn.datasets.make_swiss_roll(n_samples=batch_size, noise=1.0)[0]\n",
    "        data = data.astype(\"float32\")[:, [0, 2]]\n",
    "        data /= 5\n",
    "        return data\n",
    "\n",
    "    elif data == \"circles\":\n",
    "        data = sklearn.datasets.make_circles(n_samples=batch_size, factor=.5, noise=0.08)[0]\n",
    "        data = data.astype(\"float32\")\n",
    "        data *= 3\n",
    "        return data\n",
    "\n",
    "    elif data == \"moons\":\n",
    "        data = sklearn.datasets.make_moons(n_samples=batch_size, noise=0.1)[0]\n",
    "        data = data.astype(\"float32\")\n",
    "        data = data * 2 + np.array([-1, -0.2])\n",
    "        return data\n",
    "\n",
    "    elif data == \"8gaussians\":\n",
    "        scale = 4.\n",
    "        centers = [(1, 0), (-1, 0), (0, 1), (0, -1), (1. / np.sqrt(2), 1. / np.sqrt(2)),\n",
    "                   (1. / np.sqrt(2), -1. / np.sqrt(2)), (-1. / np.sqrt(2),\n",
    "                                                         1. / np.sqrt(2)), (-1. / np.sqrt(2), -1. / np.sqrt(2))]\n",
    "        centers = [(scale * x, scale * y) for x, y in centers]\n",
    "\n",
    "        dataset = []\n",
    "        for i in range(batch_size):\n",
    "            point = rng.randn(2) * 0.5\n",
    "            idx = rng.randint(8)\n",
    "            center = centers[idx]\n",
    "            point[0] += center[0]\n",
    "            point[1] += center[1]\n",
    "            dataset.append(point)\n",
    "        dataset = np.array(dataset, dtype=\"float32\")\n",
    "        dataset /= 1.414\n",
    "        return dataset\n",
    "\n",
    "    elif data == \"25gaussians\":\n",
    "        scale = 4.\n",
    "        centers = [(x/2, y/2) for x in range(-2,3) for y in range(-2,3)]\n",
    "        centers = [(scale * x, scale * y) for x, y in centers]\n",
    "\n",
    "        dataset = []\n",
    "        for i in range(batch_size):\n",
    "            point = rng.randn(2) * 0.2\n",
    "            idx = rng.randint(25)\n",
    "            center = centers[idx]\n",
    "            point[0] += center[0]\n",
    "            point[1] += center[1]\n",
    "            dataset.append(point)\n",
    "        dataset = np.array(dataset, dtype=\"float32\")\n",
    "        dataset /= 1.414\n",
    "        return dataset\n",
    "\n",
    "    elif data == \"pinwheel\":\n",
    "        radial_std = 0.3\n",
    "        tangential_std = 0.1\n",
    "        num_classes = 5\n",
    "        num_per_class = batch_size // 5\n",
    "        rate = 0.25\n",
    "        rads = np.linspace(0, 2 * np.pi, num_classes, endpoint=False)\n",
    "\n",
    "        features = rng.randn(num_classes*num_per_class, 2) \\\n",
    "            * np.array([radial_std, tangential_std])\n",
    "        features[:, 0] += 1.\n",
    "        labels = np.repeat(np.arange(num_classes), num_per_class)\n",
    "\n",
    "        angles = rads[labels] + rate * np.exp(features[:, 0])\n",
    "        rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n",
    "        rotations = np.reshape(rotations.T, (-1, 2, 2))\n",
    "\n",
    "        return 2 * rng.permutation(np.einsum(\"ti,tij->tj\", features, rotations))\n",
    "\n",
    "    elif data == \"2spirals\":\n",
    "        n = np.sqrt(np.random.rand(batch_size // 2, 1)) * 540 * (2 * np.pi) / 360\n",
    "        d1x = -np.cos(n) * n + np.random.rand(batch_size // 2, 1) * 0.5\n",
    "        d1y = np.sin(n) * n + np.random.rand(batch_size // 2, 1) * 0.5\n",
    "        x = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y)))) / 3\n",
    "        x += np.random.randn(*x.shape) * 0.1\n",
    "        return x\n",
    "\n",
    "    elif data == \"checkerboard\":\n",
    "        x1 = np.random.rand(batch_size) * 4 - 2\n",
    "        x2_ = np.random.rand(batch_size) - np.random.randint(0, 2, batch_size) * 2\n",
    "        x2 = x2_ + (np.floor(x1) % 2)\n",
    "        return np.concatenate([x1[:, None], x2[:, None]], 1) * 2\n",
    "    \n",
    "    elif data == \"rings\":\n",
    "        from scipy.stats import multivariate_normal, truncnorm\n",
    "        toy_sample = np.zeros(0).reshape(0, 2)\n",
    "\n",
    "        toy_groups = 4\n",
    "        weights = np.ones(toy_groups) / toy_groups\n",
    "        sample_group_sz = np.random.multinomial(batch_size, weights)\n",
    "\n",
    "        toy_radius = 0.8\n",
    "        toy_sd = 0.05\n",
    "        for i in range(toy_groups):\n",
    "            truncnorm_rv = truncnorm(\n",
    "                a=(0 - toy_radius * (i + 1)) / toy_sd,\n",
    "                b=np.inf,\n",
    "                loc=toy_radius * (i + 1),\n",
    "                scale=toy_sd,\n",
    "            )\n",
    "            # sample_radii = self.toy_radius*(i+1) + self.toy_sd * np.random.randn(sample_group_sz[i])\n",
    "            sample_radii = truncnorm_rv.rvs(sample_group_sz[i])\n",
    "            sample_thetas = 2 * np.pi * np.random.random(sample_group_sz[i])\n",
    "            sample_x = sample_radii.reshape(-1, 1) * np.cos(sample_thetas).reshape(\n",
    "                -1, 1\n",
    "            )\n",
    "            sample_y = sample_radii.reshape(-1, 1) * np.sin(sample_thetas).reshape(\n",
    "                -1, 1\n",
    "            )\n",
    "            sample_group = np.concatenate((sample_x, sample_y), axis=1)\n",
    "            toy_sample = np.concatenate(\n",
    "                (toy_sample, sample_group.reshape(-1, 2)), axis=0\n",
    "            )\n",
    "        return toy_sample\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OnlineToyDataset(object):\n",
    "  \"\"\"Wrapper of inf_datagen.\"\"\"\n",
    "\n",
    "  def __init__(self, data_name, vocab_size, discrete_dim):\n",
    "    self.dim = vocab_size\n",
    "    self.data_name = data_name\n",
    "    self.rng = np.random.RandomState()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.discrete_dim = discrete_dim\n",
    "\n",
    "    rng = np.random.RandomState(1)\n",
    "    samples = inf_train_gen(self.data_name, rng, 5000)\n",
    "    self.f_scale = np.max(np.abs(samples)) + 1\n",
    "    self.int_scale = self.dim ** (discrete_dim // 2) / (self.f_scale + 1)\n",
    "    # print(\"f_scale,\", self.f_scale, \"int_scale,\", self.int_scale)\n",
    "\n",
    "  def gen_batch(self, batch_size):\n",
    "    return inf_train_gen(self.data_name, self.rng, batch_size)\n",
    "\n",
    "  def data_gen(self, batch_size):\n",
    "    while True:\n",
    "      yield self.gen_batch(batch_size)\n",
    "\n",
    "def float2base(samples, discrete_dim, f_scale, int_scale, vocab_size):\n",
    "    base_list = []\n",
    "    for i in range(samples.shape[0]):\n",
    "        x, y = (samples[i] + f_scale) / 2 * int_scale\n",
    "        bx, by = compress(x, discrete_dim, vocab_size), compress(y, discrete_dim, vocab_size)\n",
    "        base_list.append(np.array(list(bx + by), dtype=int))\n",
    "    return np.array(base_list)\n",
    "\n",
    "def base2float(samples, discrete_dim, f_scale, int_scale, vocab_size):\n",
    "    \"\"\"Convert binary to float numpy.\"\"\"\n",
    "    floats = []\n",
    "    for i in range(samples.shape[0]):\n",
    "        s = ''\n",
    "        for j in range(samples.shape[1]):\n",
    "            s += str(samples[i, j])\n",
    "        x, y = s[:discrete_dim//2], s[discrete_dim//2:]\n",
    "        x, y = recover(x, vocab_size), recover(y, vocab_size)\n",
    "        x = x / int_scale * 2. - f_scale\n",
    "        y = y / int_scale * 2. - f_scale\n",
    "        floats.append((x, y))\n",
    "    return np.array(floats)\n",
    "\n",
    "def compress(x, discrete_dim, vocab_size):\n",
    "    bx = np.base_repr(int(abs(x)), base=vocab_size).zfill(discrete_dim // 2)\n",
    "    return bx\n",
    "\n",
    "def recover(bx, vocab_size):\n",
    "    x = int(bx, vocab_size)\n",
    "    return x\n",
    "\n",
    "def get_batch_data(db, batch_size):\n",
    "    discrete_dim = 16\n",
    "    vocab_size = 5\n",
    "\n",
    "    bx = db.gen_batch(batch_size)\n",
    "    f_scale = db.f_scale\n",
    "    int_scale = db.int_scale\n",
    "\n",
    "    bx = float2base(bx, discrete_dim, f_scale, int_scale, vocab_size)\n",
    "    return bx\n",
    "\n",
    "def plot_heat(score_func, db, device):\n",
    "    score_func.eval()\n",
    "    w = 100\n",
    "    x = np.linspace(-db.f_scale, db.f_scale, w)\n",
    "    y = np.linspace(-db.f_scale, db.f_scale, w)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    xx = np.reshape(xx, [-1, 1])\n",
    "    yy = np.reshape(yy, [-1, 1])\n",
    "    heat_samples = float2base(np.concatenate((xx, yy), axis=-1), db.discrete_dim, db.f_scale, db.int_scale, db.vocab_size)\n",
    "    heat_samples = torch.from_numpy(np.float32(heat_samples)).to(device)\n",
    "    heat_score = F.softmax(-score_func(heat_samples).view(1, -1), dim=-1)\n",
    "    a = heat_score.view(w, w).data.cpu().numpy()\n",
    "    a = np.flip(a, axis=0)\n",
    "    plt.imshow(a)\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributions as dists\n",
    "from torch.distributions.bernoulli import Bernoulli\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Swish, self).__init__()\n",
    "        self.beta = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(self.beta * x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        if isinstance(hidden_dims, str):\n",
    "            hidden_dims = list(map(int, hidden_dims.split(\"-\")))\n",
    "        assert len(hidden_dims)\n",
    "        hidden_dims = [input_dim] + hidden_dims\n",
    "        self.output_size = hidden_dims[-1]\n",
    "\n",
    "        list_layers = []\n",
    "\n",
    "        for i in range(1, len(hidden_dims)):\n",
    "            list_layers.append(nn.Linear(hidden_dims[i - 1], hidden_dims[i]))\n",
    "            if i + 1 < len(hidden_dims):  # not the last layer\n",
    "                list_layers.append(Swish())\n",
    "\n",
    "        self.main = nn.Sequential(*list_layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.main(z)\n",
    "        return x\n",
    "\n",
    "class MLPScore(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(MLPScore, self).__init__()\n",
    "        self.mlp = MLP(input_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, z):\n",
    "        raw_score = self.mlp(z.float())\n",
    "        return raw_score\n",
    "\n",
    "class EBM(nn.Module):\n",
    "    def __init__(self, net, emb_dim=4, vocab_size=5):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Linear(self.vocab_size, self.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''we define p(x) = exp(-f(x)) / Z, the output of net is f(x)\n",
    "        x: [bs, dim, vocab_size] or [bs, dim], we use one-hot encoding\n",
    "        '''\n",
    "        if len(x.shape) == 2:\n",
    "            x = F.one_hot(x.long(), num_classes=self.vocab_size).float()\n",
    "            emb = self.embedding(x)\n",
    "        else:\n",
    "            emb = self.embedding(x)\n",
    "        emb = emb.view(x.shape[0], -1)\n",
    "\n",
    "        logp = self.net(emb).squeeze()\n",
    "        return logp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_cat_grid(samples, num_classes, m_particles=32):\n",
    "    device = samples.device\n",
    "    bs, C = samples.shape # C is the number of categorical entries\n",
    "\n",
    "    # number of perturbed columns\n",
    "    l = 1\n",
    "    assert l == 1, 'Only one perturbed column is supported'\n",
    "\n",
    "    # Per sample noise\n",
    "    noise = torch.rand(bs, C, device = device)\n",
    "    ids_perm = torch.argsort(noise, dim = -1)\n",
    "    ids_restore = torch.argsort(ids_perm, dim = -1)\n",
    "\n",
    "    samples_keep = torch.gather(samples, dim = -1, index = ids_perm[:, :-l]).unsqueeze(1).expand(bs, m_particles, -1)\n",
    "\n",
    "    pert_num_classes = torch.gather(num_classes, dim = 0, index = ids_perm[:, -l])\n",
    "\n",
    "    uniform_noise = torch.rand(bs, m_particles, device = device)\n",
    "    scaled_noise = torch.einsum('bm, b -> bm', uniform_noise, pert_num_classes)\n",
    "\n",
    "    samples_pert = scaled_noise.int().unsqueeze(-1)\n",
    "\n",
    "    pert_samples = torch.cat([samples_keep, samples_pert], dim = -1)\n",
    "    pert_samples = torch.gather(pert_samples, dim = -1, index = ids_restore.unsqueeze(1).expand(bs, m_particles, -1))\n",
    "    return pert_samples\n",
    "\n",
    "def ed_categorical(energy_net, samples, K = 5, dim=16, epsilon = 1., m_particles = 32, w_stable = 1.):\n",
    "    \"\"\" Perturbation assumes periodic structure on discrete values\"\"\"\n",
    "    device = samples.device\n",
    "    bs, dim = samples.shape\n",
    "\n",
    "    num_classes = torch.tensor([K] * dim, device=device)\n",
    "    neg_data = perturb_cat_grid(samples, num_classes, m_particles)   # [bs, m_particles, dim]\n",
    "\n",
    "    pos_energy = energy_net(samples)   # [bs]\n",
    "    neg_energy = energy_net(neg_data.view(-1, dim)).view(bs, -1)  # [bs, m_particles]\n",
    "    val = pos_energy.view(bs, 1) - neg_energy\n",
    "    if w_stable != 0:\n",
    "        val = torch.cat([val, np.log(w_stable) * torch.ones_like(val[:, :1])], dim=-1)\n",
    "    \n",
    "    loss = val.logsumexp(dim=-1).mean()\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "def training_main_loop(energy_func, db, device):\n",
    "    ema_energy_func = copy.deepcopy(energy_func)\n",
    "    ema_energy_func.to(device)\n",
    "    opt_energy = optim.Adam(energy_func.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        pbar = tqdm(range(100))\n",
    "        for it in pbar:\n",
    "            samples = get_batch_data(db, 128)\n",
    "            samples = torch.from_numpy(np.float32(samples)).to(device)\n",
    "\n",
    "            loss = ed_categorical(energy_func, samples, K=db.vocab_size, dim=db.discrete_dim)\n",
    "            \n",
    "            opt_energy.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(energy_func.parameters(), max_norm=5)\n",
    "            opt_energy.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for p, ema_p in zip(energy_func.parameters(), ema_energy_func.parameters()):\n",
    "                    ema_p.data = ema_p.data * 0.999 + p.data * (1. - 0.999)\n",
    "\n",
    "            pbar.set_description('epoch: %d, loss: %.4f' % (epoch, loss.item()))\n",
    "\n",
    "    return ema_energy_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Discrete EBM using Grid Perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 3.4812: 100%|██████████| 100/100 [00:01<00:00, 54.41it/s]\n",
      "epoch: 1, loss: 3.4539: 100%|██████████| 100/100 [00:01<00:00, 79.28it/s]\n",
      "epoch: 2, loss: 3.4354: 100%|██████████| 100/100 [00:01<00:00, 99.63it/s]\n",
      "epoch: 3, loss: 3.4552: 100%|██████████| 100/100 [00:01<00:00, 93.74it/s]\n",
      "epoch: 4, loss: 3.4621: 100%|██████████| 100/100 [00:01<00:00, 81.09it/s]\n",
      "epoch: 5, loss: 3.4033: 100%|██████████| 100/100 [00:01<00:00, 87.50it/s]\n",
      "epoch: 6, loss: 3.4388: 100%|██████████| 100/100 [00:01<00:00, 88.81it/s]\n",
      "epoch: 7, loss: 3.3874: 100%|██████████| 100/100 [00:01<00:00, 73.23it/s]\n",
      "epoch: 8, loss: 3.4208: 100%|██████████| 100/100 [00:01<00:00, 80.81it/s]\n",
      "epoch: 9, loss: 3.4277: 100%|██████████| 100/100 [00:01<00:00, 85.97it/s]\n",
      "epoch: 10, loss: 3.3848:  10%|█         | 10/100 [00:00<00:00, 95.81it/s]"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "emb_dim = 4\n",
    "vocab_size = 5\n",
    "discrete_dim = 16\n",
    "db = OnlineToyDataset('pinwheel', vocab_size, discrete_dim)\n",
    "net = MLPScore(discrete_dim*emb_dim, [256] * 3 + [1]).to(device)\n",
    "energy_func = EBM(net, emb_dim=4, vocab_size=5).to(device)\n",
    "ema_energy_func = training_main_loop(energy_func, db, device)\n",
    "plot_heat(energy_func, db, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ebm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
